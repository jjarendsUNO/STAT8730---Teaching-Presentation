---
title: "Presentation Code"
author: "Justin Arends"
date: "2022-10-13"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tidymodels)

```

# Performance Measures

- With unbalanced data, overall accuracy is often inappropriate
- True negative, true positive, weighted accuracy, G-mean, precision, recall, and F-measure

| $True\ Negative (Acc^-) = \frac{TN}{TN + FP}$
| $True\ Positive (Acc^+)= \frac{TP}{FP + FN}$
| $G-mean = (Acc^- \times Acc^+)^{1/2}$
| $Weighted\ Accuracy = \beta Acc^- + (1-\beta)Acc^-$
| $Precision = \frac{TP}{TP+FN}$
| $Recall = \frac{TP}{TP+FN} = Acc^+$
| $F-Measure = \frac{2 \times Precision \times Recall}{Precision+Recall}$    


# Measures

## Custom Measures based on the out-of-the-box in `tidymodels`

```{r}
f_meas_beta_vec <- function(truth,
                            estimate,
                            estimator = NULL,
                            na_rm = TRUE,
                            event_level = "second",
                            ...) {
  f_meas_vec(
    truth = truth, 
    estimate = estimate, 
    beta = 1, 
    estimator = estimator,
    na_rm = na_rm,
    event_level = "second",
    ...
  )
}

f_meas_beta <- function(data,
                        truth,
                        estimate,
                        estimator = NULL,
                        na_rm = TRUE,
                        event_level = "second",
                        ...) {
  metric_summarizer(
    metric_nm = "f_meas_beta", 
    metric_fn = f_meas_beta_vec, 
    data = data, 
    truth = !!enquo(truth), 
    estimate = !!enquo(estimate), 
    estimator = estimator, 
    na_rm = na_rm, 
    event_level = "second", 
    ...
  )
}


recall_second_vec <- function(truth,
                            estimate,
                            estimator = NULL,
                            na_rm = TRUE,
                            event_level = "second",
                            ...) {
  recall_vec(
    truth = truth, 
    estimate = estimate, 
    estimator = estimator,
    na_rm = na_rm,
    event_level = "second",
    ...
  )
}

recall_second <- function(data,
                        truth,
                        estimate,
                        estimator = NULL,
                        na_rm = TRUE,
                        event_level = "second",
                        ...) {
  metric_summarizer(
    metric_nm = "recall_second_vec", 
    metric_fn = recall_second_vec, 
    data = data, 
    truth = !!enquo(truth), 
    estimate = !!enquo(estimate), 
    estimator = estimator, 
    na_rm = na_rm, 
    event_level = "second", 
    ...
  )
}
test_f <- new_class_metric(f_meas_beta,"maximize")
test_recall <- new_class_metric(recall_second, "maximize")



```

### Custom Measures based on the paper

```{r Custom Metics}
metrics <- function(conf_mat){
  TP = conf_mat$table[2,2]
  FP = conf_mat$table[1,2]
  TN = conf_mat$table[1,1]
  FN = conf_mat$table[2,1]
  
  precision = TP/(TP+FP)
  TNR = TN / (TN+FP)
  TPR = TP / (TP+FN)
  
  gMean = (TNR * TPR)^(1/2)
  
  fMeasure = (2*precision*TPR)/(precision + TPR)
  
  wAccuracy = (.5*TPR) + (1-.5)*TNR
  
  return(
    tibble(
      "Measure" = c("Precision", "TNR", "TPR", "G-Mean", "F-Measure", "Weighted Accuracy"),
      "Value" = c(precision, TNR,TPR, gMean, fMeasure, wAccuracy)
    )
  )
}
```

# Modeling

## Oil Data

I'm going to start a test with oil data since it's smaller and faster to iterate over.

```{r}
oil_data <- read_csv("https://github.com/jbrownlee/Datasets/raw/master/oil-spill.csv",col_names = NULL)

# X51 is the classification column

oil_data$X50 <- as.factor(oil_data$X50)



```

Create different folds

```{r}

set.seed(1234)

oil.split <- initial_split(oil_data, strata = X50)
oil.train <- training(oil.split)
oil.test <- training(oil.split)

set.seed(456)

oil.folds <- vfold_cv(oil.train, strata = X50)

oil.boots <- bootstraps(oil.train, strata = X50, pool = .5)

```

Creating a basic recipe

```{r}
oil.recipe <- recipe(X50 ~., data = oil.train) %>% 
  update_role(X1, new_role = 'id')
oil.rf.spec <- 
  rand_forest(trees = 1000,
              mode = "classification") %>% 
  set_engine("ranger")

oil.rf.wf <- 
  workflow() %>% 
  add_recipe(oil.recipe) %>% 
  add_model(oil.rf.spec)


prep(oil.recipe) %>% juice() %>% count(X50)

```

Fitting with the basic model on the oild data. Using `tidymodel's` out of the box metrics...

```{r}
set.seed(123)
doParallel::registerDoParallel()
oil.fold.fit <- 
  oil.rf.wf %>% 
  fit_resamples(oil.boots, metrics=metric_set(accuracy, 
                                              roc_auc, 
                                              specificity,
                                              recall,
                                              f_meas,
                                              test_f,
                                              sens,
                                              recall,
                                              test_recall,
                                              precision
                                              ),
                control = control_grid(save_pred = TRUE))

oil.fold.fit %>% 
  collect_metrics()
```

They yield slightly different results, so I'll go ahead and leverage the custom metrics I build based on the paper's formula.


```{r}
oil.fold.fit %>% 
  collect_predictions() %>% 
  conf_mat(truth = X50, estimate = .pred_class) %>%
  metrics()
  
```

I think this is fairly close to the "one-sided sampling" table of the oil data. So, I'll use this structure on the Mammography data.

# Mammography
```{r}
mam.data <- read_csv("https://github.com/jbrownlee/Datasets/raw/master/mammography.csv", 
                     col_names  = NULL) %>% 
  mutate(X7 = as.factor(ifelse(X7 =="'-1'", 0, 1)))

mam.data %>% 
  count(X7)

```

## Training/Testing Splits

Divide the data into training/testing/and folds.

```{r}
set.seed(1234)

mam.split <- initial_split(mam.data, strata = X7)
mam.train <- training(mam.split)
mam.test <- training(mam.split)

set.seed(456)

mam.folds <- vfold_cv(mam.train, strata = X7)

```

All the model recipes

```{r, echo = TRUE}
mam.rec.balanced <- 
  recipe(X7 ~ ., data = mam.train) %>% 
  themis::step_upsample(X7, over_ratio = 1) 


# Baseline
mam.rec <- 
  recipe(X7 ~ ., data = mam.train)


# Smote
mam.rec.smote.100 <- 
  recipe(X7 ~., data = mam.train) %>% 
  themis::step_smote(X7, over_ratio = 1)

mam.rec.smote.200 <- 
  recipe(X7 ~., data = mam.train) %>% 
  themis::step_smote(X7, over_ratio = 2)


# Class Weights 
rf.spec_w2 <- 
  rand_forest(trees = 1000,
              mode = "classification") %>% 
  set_engine("ranger",
              class.weights = c(1,2))


rf.spec_w3 <- 
  rand_forest(trees = 1000,
              mode = "classification") %>% 
  set_engine("ranger",
              class.weights = c(1,5))


# Balanced Recipes
rf.spec.balanced.5 <- 
  rand_forest(trees = 1000,
              mode = "classification") %>% 
  set_engine("ranger",
              mtry = .5)

rf.spec.balanced.6 <- 
  rand_forest(trees = 1000,
              mode = "classification") %>% 
  set_engine("ranger",
              mtry = .6)


rf.spec.balanced.7 <- 
  rand_forest(trees = 1000,
              mode = "classification") %>% 
  set_engine("ranger",
              mtry = .7)

# Model Specifications
rf.spec <- 
  rand_forest(trees = 1000,
              mode = "classification") %>% 
  set_engine("ranger")


mam.wf <- 
  workflow() %>% 
  add_recipe(mam.rec) %>% 
  add_model(rf.spec_w2)


  
```


```{r Resampling Fits, cache = TRUE}
set.seed(123)
doParallel::registerDoParallel(parallel::makeCluster(7))

mam.fold.fit.2 <-
  mam.wf %>%
  fit_resamples(
    mam.folds,
    metrics = metric_set(
      accuracy,
      roc_auc
    ),
    control = control_grid(save_pred = TRUE,
                           save_workflow = TRUE)
  )

mam.fold.fit.3 <-
  mam.wf %>% 
  update_model(rf.spec_w3) %>%
  fit_resamples(
    mam.folds,
    metrics = metric_set(
      accuracy,
      roc_auc
    ),
    control = control_grid(save_pred = TRUE,
                           save_workflow = TRUE)
  )

mam.fold.fit.smote.100 <- 
  mam.wf %>% 
  update_recipe(mam.rec.smote.100) %>%
  update_model(rf.spec) %>% 
  fit_resamples(
    mam.folds,
    metrics = metric_set(
      accuracy,
      roc_auc
    ),
    control = control_grid(save_pred = TRUE,
                           save_workflow = TRUE)
  )

mam.fold.fit.smote.200 <- 
  mam.wf %>% 
  update_recipe(mam.rec.smote.200) %>%
  update_model(rf.spec) %>% 
  fit_resamples(
    mam.folds,
    metrics = metric_set(
      accuracy,
      roc_auc
    ),
    control = control_grid(save_pred = TRUE,
                           save_workflow = TRUE)
  )

mam.fold.fit.balanced.5 <- 
  mam.wf %>% 
  update_recipe(mam.rec.balanced) %>%
  update_model(rf.spec.balanced.5) %>% 
  fit_resamples(
    mam.folds,
    metrics = metric_set(
      accuracy,
      roc_auc
    ),
    control = control_grid(save_pred = TRUE,
                           save_workflow = TRUE)
  )

mam.fold.fit.balanced.6 <- 
  mam.wf %>% 
  update_recipe(mam.rec.balanced) %>%
  update_model(rf.spec.balanced.6) %>% 
  fit_resamples(
    mam.folds,
    metrics = metric_set(
      accuracy,
      roc_auc
    ),
    control = control_grid(save_pred = TRUE,
                           save_workflow = TRUE)
  )


mam.fold.fit.balanced.7 <- 
  mam.wf %>% 
  update_recipe(mam.rec.balanced) %>%
  update_model(rf.spec.balanced.7) %>% 
  fit_resamples(
    mam.folds,
    metrics = metric_set(
      accuracy,
      roc_auc
    ),
    control = control_grid(save_pred = TRUE,
                           save_workflow = TRUE)
  )

```

```{r Confusion Matrices, echo = FALSE}
mam.confusion.matrix.2 <- 
  mam.fold.fit.2 %>% 
  collect_predictions() %>% 
  conf_mat(truth = X7, 
           estimate = .pred_class) 


mam.confusion.matrix.3 <- 
  mam.fold.fit.3 %>% 
  collect_predictions() %>% 
  conf_mat(truth = X7, 
           estimate = .pred_class) 

mam.confusion.matrix.smote.100 <- 
  mam.fold.fit.smote.100 %>% 
  collect_predictions() %>% 
  conf_mat(truth = X7, 
           estimate = .pred_class) 

mam.confusion.matrix.smote.200 <- 
  mam.fold.fit.smote.200 %>% 
  collect_predictions() %>% 
  conf_mat(truth = X7, 
           estimate = .pred_class) 


mam.confusion.matrix.balanced.5 <- 
  mam.fold.fit.balanced.5 %>% 
  collect_predictions() %>% 
  conf_mat(truth = X7, 
           estimate = .pred_class) 

mam.confusion.matrix.balanced.6 <- 
  mam.fold.fit.balanced.6 %>% 
  collect_predictions() %>% 
  conf_mat(truth = X7, 
           estimate = .pred_class) 

mam.confusion.matrix.balanced.7 <- 
  mam.fold.fit.balanced.7 %>% 
  collect_predictions() %>% 
  conf_mat(truth = X7, 
           estimate = .pred_class) 

```

## Joining all the different models into a single table to compare

```{r, Metrics Table}
metric_table <- 
  do.call(
    "rbind",
    list(
      metrics(mam.confusion.matrix.smote.100) %>% mutate(model = "SMOTE 100"),
      metrics(mam.confusion.matrix.smote.200) %>% mutate(model = "SMOTE 200"),
      metrics(mam.confusion.matrix.2) %>% mutate(model = "Weighted 2:1"),
      metrics(mam.confusion.matrix.3) %>% mutate(model = "Weighted 3:1"),
      metrics(mam.confusion.matrix.balanced.5) %>% mutate(model = "Balanced .5"),
      metrics(mam.confusion.matrix.balanced.6) %>% mutate(model = "Balanced .6"),
      metrics(mam.confusion.matrix.balanced.7) %>% mutate(model = "Balanced .7")
    )
  ) 

metric_table %>% 
  ggplot(aes(Value, model))+
  geom_col()+
  facet_wrap(~Measure, scales = "free_x")


metric_table %>% 
  pivot_wider(id_cols = model,
              values_from = Value,
              names_from = Measure)
```

## SMOTEBoost
```{r}
library(ebmc)

#Models
mam.train2 <- as.data.frame(mam.train)  

mam.fit.smote.100 <-
  sbo(
    X7 ~ .,
    data = mam.train2,
    size = 3,
    alg = "rf",
    over = 100,
    svm.ker = "sigmoid",
    rf.ntree = 500
  )
mam.fit.smote.300 <-
  sbo(
    X7 ~ .,
    data = mam.train2,
    size = 3,
    alg = "rf",
    over = 300,
    svm.ker = "sigmoid",
    rf.ntree = 500
  )


# Predictions
pred.mam.fit.smote.100 <- 
  predict.modelBst(mam.fit.smote.100, 
                   newdata = mam.test[,-7], 
                   type = "class")

pred.mam.fit.smote.300 <- 
  predict.modelBst(mam.fit.smote.300, 
                   newdata = mam.test[,-7], 
                   type = "class")

# confusion matrices
smote.100.conf.mat <- tibble("truth" = mam.test$X7,
                             "prediction" = pred.mam.fit.smote.100) %>% 
  conf_mat(truth = truth, 
           estimate = prediction) 

smote.300.conf.mat <- tibble("truth" = mam.test$X7,
                             "prediction" = pred.mam.fit.smote.300) %>% 
  conf_mat(truth = truth, 
           estimate = prediction) 


```

### For loop for SMOTE Boost
```{r}
library(ebmc)
results100 <- tibble()


for (i in 1:10) {
  temp_train <-
    mam.folds$splits[[i]] %>% 
    training() %>% 
    as.data.frame()
  
  temp_test <- 
    mam.folds$splits[[i]] %>% 
    testing() %>% 
    as.data.frame()
  
  
  temp_smote100 <-
    sbo(
      X7 ~ .,
      data = temp_train,
      size = 3,
      alg = "rf",
      over = 100,
      svm.ker = "sigmoid",
      rf.ntree = 500
    )
  
  temp_preds <- predict.modelBst(temp_smote100,
                                 newdata = temp_test,
                                 type = "class")
  
  temp_truths <- temp_test$X7
  
  temp_results <- tibble(X7 = temp_truths,
                         .preds = temp_preds,
                         Model = paste0("Smote100 - Fold", i))
  
  
  results100 <- rbind(results100, temp_results)
}

conf_mat_smote100 <- 
  conf_mat(results100,
           X7, .preds)
```

```{r}
results300 <- tibble()

for (i in 1:10) {
  temp_train <-
    mam.folds$splits[[i]] %>% 
    training() %>% 
    as.data.frame()
  
  temp_test <- 
    mam.folds$splits[[i]] %>% 
    testing() %>% 
    as.data.frame()
  
  
  temp_smote300 <-
    sbo(
    X7 ~ .,
    data = temp_test,
    size = 3,
    alg = "rf",
    over = 300,
    svm.ker = "sigmoid",
    rf.ntree = 500
  )
  
  temp_preds <- predict.modelBst(temp_smote300,
                                 newdata = temp_test,
                                 type = "class")
  
  temp_truths <- temp_test$X7
  
  temp_results <- tibble(X7 = temp_truths,
                         .preds = temp_preds,
                         Model = paste0("Smote300 - Fold", i))
  
  
  results300 <- rbind(results300, temp_results)
}

conf_mat_smote300 <- 
  conf_mat(results300,
           X7, .preds)


```


```{r}
metric_table <- 
  do.call(
    "rbind",
    list(
      metrics(mam.confusion.matrix.smote.100) %>% mutate(model = "SMOTE 100"),
      metrics(mam.confusion.matrix.smote.200) %>% mutate(model = "SMOTE 200"),
      metrics(mam.confusion.matrix.2) %>% mutate(model = "Weighted 2:1"),
      metrics(mam.confusion.matrix.3) %>% mutate(model = "Weighted 3:1"),
      metrics(mam.confusion.matrix.balanced.5) %>% mutate(model = "Balanced .5"),
      metrics(mam.confusion.matrix.balanced.6) %>% mutate(model = "Balanced .6"),
      metrics(mam.confusion.matrix.balanced.7) %>% mutate(model = "Balanced .7"),
      metrics(conf_mat_smote100) %>% mutate(model = "SMOTEBOOST 100"),
      metrics(conf_mat_smote300) %>% mutate(model = "SMOTEBOOST 300")

    )
  ) 

metric_table %>% 
  ggplot(aes(Value, model))+
  geom_col()+
  facet_wrap(~Measure, scales = "free_x")


metric_table %>% 
  pivot_wider(id_cols = model,
              values_from = Value,
              names_from = Measure)

```

